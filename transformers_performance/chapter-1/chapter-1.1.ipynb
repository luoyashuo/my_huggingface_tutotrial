{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "seq_len, dataset_size = 512, 512\n",
    "dummy_data = {\n",
    "    \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "    \"labels\": np.random.randint(0, 1, (dataset_size)),\n",
    "}\n",
    "ds = Dataset.from_dict(dummy_data)\n",
    "ds.set_format(\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 311 MB.\n"
     ]
    }
   ],
   "source": [
    "# 查看空闲 GPU 内存\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernels 是用于执行底层 GPU 上数学运算的函数，我们调用kernel函数对存储在GPU内存中的数据进行计算。每种神经网络层都需要不同的 kernels 来执行其特定的计算操作，如卷积核函数、激活函数等，所以CUDA编程的核心其实也就是如何合理的划分数据并且针对数据结构编写高效的kernel函数。\n",
    "当一个模型加载到 GPU 时，与该模型相关的计算 kernels 也会被加载到 GPU 上，这样可以避免每次执行运算时都重复加载 kernels，提高运算效率。但需要注意的是，加载 kernels 会占用 GPU 存储空间，通常占用大约1-2GB的内存。因此，即使加载一个微小的张量到 GPU 上，也会触发 kernels 的加载，并且你可以通过观察 GPU 显存的使用来查看 kernels 占用的内存大小。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 311 MB.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.ones((1, 1)).to(\"cuda\")\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 1599 MB.\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"/mnt/bn/lys-lq/HF_Caches/bert-large-uncased\" # 运行后修改\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path).to(\"cuda\")\n",
    "print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 10 14:20:49 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.129.06   Driver Version: 470.129.06   CUDA Version: 12.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:1A:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    73W / 300W |   1599MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:1B:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    75W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练模型，看看显存变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.143, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-10 14:20:54.952 n128-099-069:21338:21338 [0] NCCL INFO cudaDriverVersion 12020\n",
      "2024-10-10 14:20:54.953 n128-099-069:21338:21338 [0] NCCL INFO NCCL_SOCKET_FAMILY set by environment to AF_INET6\n",
      "2024-10-10 14:20:54.953 n128-099-069:21338:21338 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\n",
      "2024-10-10 14:20:54.953 n128-099-069:21338:21338 [0] NCCL INFO Bootstrap : Using eth0:fdbd:dc03:1:334::69<0>\n",
      "2024-10-10 14:20:54.954 n128-099-069:21338:21338 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation\n",
      "NCCL version 2.19.3-2+cuda12.2\n",
      "2024-10-10 14:20:55.334 n128-099-069:21338:24233 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.\n",
      "2024-10-10 14:20:55.364 n128-099-069:21338:24233 [1] NCCL INFO NCCL_SOCKET_FAMILY set by environment to AF_INET6\n",
      "2024-10-10 14:20:55.364 n128-099-069:21338:24233 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\n",
      "2024-10-10 14:20:55.365 n128-099-069:21338:24233 [1] NCCL INFO NCCL_IB_HCA set to mlx5_2:1\n",
      "2024-10-10 14:20:55.382 n128-099-069:21338:24233 [1] NCCL INFO NET/IB : Using [0]mlx5_2:1/RoCE [RO]; OOB eth0:fdbd:dc03:1:334::69<0>\n",
      "2024-10-10 14:20:55.383 n128-099-069:21338:24233 [1] NCCL INFO Using non-device net plugin version 0\n",
      "2024-10-10 14:20:55.383 n128-099-069:21338:24233 [1] NCCL INFO Using network IB\n",
      "2024-10-10 14:20:55.383 n128-099-069:21338:24232 [0] NCCL INFO Using non-device net plugin version 0\n",
      "2024-10-10 14:20:55.383 n128-099-069:21338:24232 [0] NCCL INFO Using network IB\n",
      "2024-10-10 14:20:55.384 n128-099-069:21338:24232 [0] NCCL INFO comm 0x133a0b60 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1a000 commId 0xe54db08cd2d95676 - Init START\n",
      "2024-10-10 14:20:55.384 n128-099-069:21338:24233 [1] NCCL INFO comm 0x133a4660 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 1b000 commId 0xe54db08cd2d95676 - Init START\n",
      "2024-10-10 14:20:55.399 n128-099-069:21338:24232 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\n",
      "2024-10-10 14:20:55.402 n128-099-069:21338:24233 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\n",
      "2024-10-10 14:20:55.403 n128-099-069:21338:24233 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0\n",
      "2024-10-10 14:20:55.403 n128-099-069:21338:24232 [0] NCCL INFO Channel 00/02 :    0   1\n",
      "2024-10-10 14:20:55.403 n128-099-069:21338:24233 [1] NCCL INFO P2P Chunksize set to 524288\n",
      "2024-10-10 14:20:55.403 n128-099-069:21338:24232 [0] NCCL INFO Channel 01/02 :    0   1\n",
      "2024-10-10 14:20:55.403 n128-099-069:21338:24232 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\n",
      "2024-10-10 14:20:55.403 n128-099-069:21338:24232 [0] NCCL INFO P2P Chunksize set to 524288\n",
      "2024-10-10 14:20:55.412 n128-099-069:21338:24233 [1] NCCL INFO Ring Channel 00/0 : 1[1] -> 0[0] via P2P/direct pointer\n",
      "2024-10-10 14:20:55.412 n128-099-069:21338:24232 [0] NCCL INFO Ring Channel 00/0 : 0[0] -> 1[1] via P2P/direct pointer\n",
      "2024-10-10 14:20:55.412 n128-099-069:21338:24233 [1] NCCL INFO Ring Channel 01/0 : 1[1] -> 0[0] via P2P/direct pointer\n",
      "2024-10-10 14:20:55.413 n128-099-069:21338:24232 [0] NCCL INFO Ring Channel 01/0 : 0[0] -> 1[1] via P2P/direct pointer\n",
      "2024-10-10 14:20:55.439 n128-099-069:21338:24232 [0] NCCL INFO Connected all rings\n",
      "2024-10-10 14:20:55.439 n128-099-069:21338:24232 [0] NCCL INFO Connected all trees\n",
      "2024-10-10 14:20:55.439 n128-099-069:21338:24233 [1] NCCL INFO Connected all rings\n",
      "2024-10-10 14:20:55.439 n128-099-069:21338:24233 [1] NCCL INFO Connected all trees\n",
      "2024-10-10 14:20:55.439 n128-099-069:21338:24233 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "2024-10-10 14:20:55.439 n128-099-069:21338:24233 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "2024-10-10 14:20:55.439 n128-099-069:21338:24232 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512\n",
      "2024-10-10 14:20:55.439 n128-099-069:21338:24232 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "2024-10-10 14:20:55.482 n128-099-069:21338:24233 [1] NCCL INFO comm 0x133a4660 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId 1b000 commId 0xe54db08cd2d95676 - Init COMPLETE\n",
      "2024-10-10 14:20:55.482 n128-099-069:21338:24232 [0] NCCL INFO comm 0x133a0b60 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1a000 commId 0xe54db08cd2d95676 - Init COMPLETE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 46.475, 'train_samples_per_second': 11.017, 'train_steps_per_second': 1.377, 'train_loss': 0.019991444423794746, 'epoch': 1.0}\n",
      "Time: 46.48\n",
      "Samples/second: 11.02\n",
      "GPU memory occupied: 11493 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看到,与仅仅将模型加载到GPU相比，模型训练需要使用更多的内存。这是因为训练过程中有许多组件都使用了GPU内存:\n",
    "\n",
    "模型权重\n",
    "fp32：每个参数4字节\n",
    "混合精度训练：每个参数6字节(同时在内存中维护fp32版本和fp16版本)\n",
    "优化器状态（optimizer states）\n",
    "normal AdamW：每个参数8字节(维护一阶动量和二阶动量2个状态，都是fp32版本)\n",
    "8-bit AdamW(如bitsandbytes)：每个参数2字节（也是两个状态，但都是int8版本）\n",
    "SGD：每个参数4字节(仅维护1个状态)\n",
    "梯度： 每个参数4字节（无论是否启用混合精度训练，梯度始终以fp32存储)\n",
    "Forward Activations：用于梯度计算，其大小取决于许多因素，比如序列长度、隐含层大小和批量大小。\n",
    "临时缓存：各种暂存变量,一旦计算完成就会释放，但当时可能需要额外内存，所以也可能导致OOM。编程时必须考虑这些临时变量，及时释放不再需要的变量。\n",
    "特定功能的内存：除了以上的消耗，可能还有特殊的内存需求。例如，使用束搜索（beam search）生成文本时，需要维护输入和输出的多个副本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiger/.local/lib/python3.9/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    **default_args,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'fp16'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/bn/lys-lq/my_huggingface_tutotrial/transformers_performance/chapter-1/chapter-1.1.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/lys-lq/my_huggingface_tutotrial/transformers_performance/chapter-1/chapter-1.1.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     model\u001b[39m.\u001b[39mgradient_checkpointing_enable()     \u001b[39m# 启用梯度检查点\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/lys-lq/my_huggingface_tutotrial/transformers_performance/chapter-1/chapter-1.1.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# 初始化Accelerator时指定启用混合精度训练\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/lys-lq/my_huggingface_tutotrial/transformers_performance/chapter-1/chapter-1.1.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m accelerator \u001b[39m=\u001b[39m Accelerator(fp16\u001b[39m=\u001b[39;49mtraining_args\u001b[39m.\u001b[39;49mfp16)\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/lys-lq/my_huggingface_tutotrial/transformers_performance/chapter-1/chapter-1.1.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# 启用adamw_bnb_8bit\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://icube%2Bicube/mnt/bn/lys-lq/my_huggingface_tutotrial/transformers_performance/chapter-1/chapter-1.1.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m model, optimizer, dataloader \u001b[39m=\u001b[39m accelerator\u001b[39m.\u001b[39mprepare(model, adam_bnb_optim, dataloader)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'fp16'"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "dataloader = DataLoader(ds, batch_size=training_args.per_device_train_batch_size)\n",
    "\n",
    "if training_args.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()     # 启用梯度检查点\n",
    "\n",
    "# 初始化Accelerator时指定启用混合精度训练\n",
    "accelerator = Accelerator(fp16=training_args.fp16)\n",
    "# 启用adamw_bnb_8bit\n",
    "model, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)\n",
    "\n",
    "model.train()\n",
    "for step, batch in enumerate(dataloader, start=1):\n",
    "    loss = model(**batch).loss\n",
    "    loss = loss / training_args.gradient_accumulation_steps\n",
    "    # 调用accelerator进行反向传播\n",
    "    accelerator.backward(loss)\n",
    "    if step % training_args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "fileId": "72c83960-b65d-42d9-8b77-49bd6231b312",
  "filePath": "/mnt/bn/lys-lq/my_huggingface_tutotrial/transformers_performance/chapter-1/chapter-1.1.ipynb",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
